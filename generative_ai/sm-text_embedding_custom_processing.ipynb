{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre/Post-Processing for Hugging Face (HF) Text Embeddings Inference (TEI)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this demo notebook, we demonstrate how to implement pre/post-processing logic for [HF TEI](https://huggingface.co/docs/text-embeddings-inference/en/index) use cases. While it provides improved performance and convenient SageMaker integrations, the [SageMaker TEI image](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-support-policy.html#pre-built-containers-support-policy-dlc) does not support custom pre/post-processing logic out-of-the-box. For customers looking to customize their logic, we show how to leverage the [SageMaker Transformers Image](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers) to load and invoke a HF TEI model with a custom InferenceSpec so that customers can still pre/post-process their results from their SageMaker endpoint(s).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook was tested in region `us-west-2` with kernel `conda_python3 (3.10.15 | packaged by conda-forge | (main, Sep 20 2024, 16:37:05) [GCC 13.3.0])` and uses the following (versioned) resources:\n",
    "\n",
    "| Resource | Value |\n",
    "| :-------- | :----- |\n",
    "| TEI Model | jinaai/jina-embeddings-v2-small-en |\n",
    "| TEI Image (as a control for performance testing) | 246618743249.dkr.ecr.us-west-2.amazonaws.com/tei-cpu:2.0.1-tei1.4.0-cpu-py310-ubuntu22.04 |\n",
    "| Transformers Image | 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-cpu-py310-ubuntu22.04 |\n",
    "| Instance Type | ml.m5.4xlarge |\n",
    "\n",
    "- The accounts must match the account from which the region is pulled from. This varies based on resource. For example, `246618743249` is the account providing the TEI Image for the `us-west-2` region. Please contact the resource providers for more info about target resources. \n",
    "- The images used are based on `Python3.10`. The **image versions must match the PySDK version** for reasons related to [pickling](https://docs.python.org/3/library/pickle.html) performed by the PySDK.\n",
    "- The images used are for target instance type `ml.m5.4xlarge` i.e. `CPU` . Please be sure to use the appropriate TEI and Transformer images for target hardware.\n",
    "- The TEI model used must fit on the target instance type; otherwise, the inferences will fail (even if the endpoint still reports \"online\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.232.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.46.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.35.46)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.25.5)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.0)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.10)\n",
      "Requirement already satisfied: sagemaker-mlflow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.1.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.66.5)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.10.6)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.46 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.35.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.20.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (2024.8.30)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.9.2)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (13.8.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2024.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.9)\n",
      "INFO: pip is looking at multiple versions of pathos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pathos (from sagemaker)\n",
      "  Downloading pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.5)\n",
      "Requirement already satisfied: mlflow>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-mlflow->sagemaker) (2.17.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.17.0)\n",
      "Requirement already satisfied: Flask<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.13.3)\n",
      "Requirement already satisfied: graphene<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.4)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.9.2)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.5.2)\n",
      "Requirement already satisfied: scipy<2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.14.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.0.35)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (23.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (8.1.7)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.36.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.43)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.27.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.18.0)\n",
      "Requirement already satisfied: Mako in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.3.6)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.8.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (10.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.35.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.0.11)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.48b0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.7.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.6.1)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, pathos, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.17\n",
      "    Uninstalling multiprocess-0.70.17:\n",
      "      Successfully uninstalled multiprocess-0.70.17\n",
      "  Attempting uninstall: pathos\n",
      "    Found existing installation: pathos 0.3.3\n",
      "    Uninstalling pathos-0.3.3:\n",
      "      Successfully uninstalled pathos-0.3.3\n",
      "Successfully installed datasets-3.1.0 dill-0.3.8 multiprocess-0.70.16 pathos-0.3.2 xxhash-3.5.0\n",
      "3.10.15 | packaged by conda-forge | (main, Sep 20 2024, 16:37:05) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker numpy transformers datasets --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role\n",
    "\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "ROLE = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants for our Intended Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TEI_MODEL = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "ROLE = \"arn:aws:iam::987461069402:role/service-role/AmazonSageMaker-ExecutionRole-20240417T141916\"\n",
    "INSTANCE_TYPE = \"ml.m5.4xlarge\"\n",
    "TEI_IMAGE = (\n",
    "    \"246618743249.dkr.ecr.us-west-2.amazonaws.com/tei-cpu:2.0.1-tei1.4.0-cpu-py310-ubuntu22.04\"\n",
    ")\n",
    "TRANSFORMERS_IMAGE = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-cpu-py310-ubuntu22.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Helper Functions to Deploy Endpoints\n",
    "\n",
    "This notebook will deploy two different endpoints:\n",
    "1. Endpoint using standard TEI image (as a control to compare against)\n",
    "2. Endpoint using Transformers image\n",
    "\n",
    "To simplify this, we define a few helper functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **deploy()**\n",
    "\n",
    "To deploy a SageMaker endpoint, we need to first build a SageMaker Model. Since we will later use [InferenceSpec](https://sagemaker.readthedocs.io/en/v2.208.0/api/inference/model_builder.html#sagemaker.serve.spec.inference_spec.InferenceSpec), we should use the SageMaker Python SDK constrcut `ModelBuilder` to build a Model (rather than directly use the `Model` construct, which does not support defining a custom InferenceSpec)\n",
    "\n",
    "When defining a new model with `ModelBuilder`, it requires a `SchemaBuilder` to understand the input and output data types. This is essential for properly (de)serializing the data. `SchemaBuilder` is able to infer the kind of (de)serializers to use by providing example inputs. For the TEI case, the input and output look like:\n",
    "- Input: a (stringified) JSON object with the sentences to get embeddings for\n",
    "- Output: a (nested) lists, which represents the embeddings. \n",
    "\n",
    "The exact shapes and names used within these samples are not critical since these are just used to infer the (de)serializers and not reshape or fit actual inputs/outputs into.\n",
    "\n",
    "Below, we use some Python shorthand involving `**` to define default arguments and allow for overrides if they are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "import sagemaker\n",
    "\n",
    "sagemaker.model.FrameworkModel\n",
    "\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "\n",
    "\n",
    "def deploy(model_builder_kwargs={}):\n",
    "    model = ModelBuilder(\n",
    "        **{\n",
    "            **dict(\n",
    "                role_arn=ROLE,\n",
    "                schema_builder=SchemaBuilder(\n",
    "                    json.dumps({\"inputs\": [\"hello\", \"world\"]}), [[1, 2, 3], [4, 5, 6]]\n",
    "                ),\n",
    "                env_vars={\n",
    "                    \"TS_DISABLE_TOKEN_AUTHORIZATION\": \"true\"  # See https://github.com/pytorch/serve/blob/master/docs/README.md\n",
    "                },\n",
    "            ),\n",
    "            **model_builder_kwargs,  # allow for overridding these arguments if desired\n",
    "        }\n",
    "    ).build()\n",
    "    endpoint = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=INSTANCE_TYPE,\n",
    "    )\n",
    "    return (model, endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **clean()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(model, endpoint):\n",
    "    try:\n",
    "        endpoint.delete_endpoint()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        model.delete_model()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endpoint-Specific Usage of Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TEI Image (as-is, used as control to compare against)\n",
    "\n",
    "The SageMaker Python SDK and SageMaker TEI image make it convenient to simply specify the intended `HF_TEI_MODEL` with the SageMaker `TEI_IMAGE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def deploy_tei():\n",
    "    return deploy(\n",
    "        model_builder_kwargs=dict(\n",
    "            name=f\"tei-{int(time.time())}\",\n",
    "            image_uri=TEI_IMAGE,\n",
    "            model=HF_TEI_MODEL,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformers Image\n",
    "\n",
    "Since we will be defining custom logic using the Transformers Image, we need to provide our own [`InferenceSpec`](https://sagemaker.readthedocs.io/en/v2.208.0/api/inference/model_builder.html#sagemaker.serve.spec.inference_spec.InferenceSpec). This construct requires the following implementations:\n",
    "- `load()` : how to load your intended model\n",
    "- `invoke()` : how to call your loaded model\n",
    "\n",
    "To do this, we create our own subclass of `InferenceSpec` called `CustomerInferenceSpec`. Then, we provide our implementations of each by overriding the expected methods with the same signature in the base class. \n",
    "\n",
    "##### **load()**\n",
    "\n",
    "[We can load our TEI model `jinaai/jina-embeddings-v2-small-en` using the transformers library as documented by the HF model owners](https://huggingface.co/jinaai/jina-embeddings-v2-base-en#usage).\n",
    "\n",
    "##### **invoke()**\n",
    "\n",
    "[We can invoke our loaded TEI model `jinaai/jina-embeddings-v2-small-en` with a single `encode()` call as documented by the HF model owners](https://huggingface.co/jinaai/jina-embeddings-v2-base-en#usage). \n",
    "\n",
    "Note: The inputs to `encode()` must be extracted from the input data sent to the endpoint. This is done with a simple JSON load and reference to `inputs`, which is what the endpoint was invoked with.\n",
    "\n",
    "##### **Dependencies**\n",
    "The SageMaker Python SDK can auto-detect dependencies at the class level of an InferenceSpec. It will install them in the intended serving container while provisioning our endpoint. \n",
    "\n",
    "For this demo, however, we would like a specific version of the HF `transformers` library that complies with the version of Python and other dependencies already in our target image. Therefore, we opt to use the `dependencies` parameter explicitly to specify a particular version of `transformers` to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.spec.inference_spec import InferenceSpec\n",
    "\n",
    "\n",
    "def deploy_transformer():\n",
    "    class CustomerInferenceSpec(InferenceSpec):\n",
    "        def load(self, model_dir):\n",
    "            from transformers import AutoModel\n",
    "\n",
    "            return AutoModel.from_pretrained(HF_TEI_MODEL, trust_remote_code=True)\n",
    "\n",
    "        def invoke(self, x, model):\n",
    "            return model.encode(json.loads(x)[\"inputs\"])\n",
    "\n",
    "        def preprocess(self, input_data):\n",
    "            return json.loads(input_data)[\"inputs\"]\n",
    "\n",
    "        def postprocess(self, predictions):\n",
    "            assert predictions is not None\n",
    "            return predictions\n",
    "\n",
    "    return deploy(\n",
    "        dict(\n",
    "            name=f\"transformers-{int(time.time())}\",\n",
    "            image_uri=TRANSFORMERS_IMAGE,\n",
    "            inference_spec=CustomerInferenceSpec(),\n",
    "            dependencies={\n",
    "                \"custom\": [\n",
    "                    \"transformers==4.38.0\"  # so we don't override the DLC dependency versions\n",
    "                ],\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Demo\n",
    "\n",
    "In order to run the demo, we will need to\n",
    "1. deploy our (models and) endpoints\n",
    "2. invoke against our (models and) endpoints\n",
    "3. clean up our (models and) endpoints \n",
    "\n",
    "We can define a just few more helpers for better reuse and customizations if needed in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions to Run This Specific Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **invoke_many()**\n",
    "\n",
    "This is a synchronous function that will be sent to a thread using asyncio. \n",
    "\n",
    "It a simple invoker used to invoke against the endpoint given a list of samples to call with.\n",
    "\n",
    "We intentionally do not use batching here for a basic performance test later in this notebook.\n",
    "\n",
    "Note: This is tightly-coupled with our sample [dataset](https://huggingface.co/datasets/sentence-transformers/stsb). Specfically, the way we run invocations with `predict` against our endpoints is based on how the dataset is structured. Each sample has two sentences. See dataset page for more info.\n",
    "\n",
    "Note: The `initial_args` are needed only for the endpoint with the TEI Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_many(endpoint, samples):  # intentionally not batching\n",
    "    results = []\n",
    "    for sample in samples:\n",
    "        start = time.perf_counter()\n",
    "        res = endpoint.predict(\n",
    "            json.dumps({\"inputs\": [sample[\"sentence1\"], sample[\"sentence2\"]]}),\n",
    "            initial_args=(\n",
    "                {\"ContentType\": \"application/json\"} if \"tei\" in endpoint.endpoint_name else None\n",
    "            ),\n",
    "        )\n",
    "        end = time.perf_counter()\n",
    "        results.append(\n",
    "            {\n",
    "                \"latency\": end - start,\n",
    "                \"embeddings\": res,\n",
    "                \"endpoint\": endpoint.endpoint_name,\n",
    "                \"sample\": sample,\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **basic_performance_test()**\n",
    "\n",
    "This is an async function that will leverage asyncio to create, invoke, and clean up endpoints.\n",
    "\n",
    "We use asyncio here for better concurrency since the endpoint (de)provisioning can take a long, indeterminate amount of time.\n",
    "\n",
    "In this demo, we will \n",
    "1. Create two endpoints:\n",
    "    1. One with TEI Image \n",
    "    2. One with Transformers Image\n",
    "2. Load sample sentences from the [`sentence-transformers/stsb` dataset](https://huggingface.co/datasets/sentence-transformers/stsb).\n",
    "3. Invoke each endpoint with the samples.\n",
    "4. Analyze the latencies of the sample invocations.\n",
    "5. Clean up all of the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import pprint\n",
    "\n",
    "\n",
    "async def basic_performance_test():\n",
    "    deployments = []\n",
    "    try:\n",
    "        ######################################################\n",
    "        # Deploy\n",
    "        ######################################################\n",
    "        print(\"Deploying endpoints...\")\n",
    "        deployments = await asyncio.gather(\n",
    "            *[asyncio.to_thread(deploy_tei), asyncio.to_thread(deploy_transformer)]\n",
    "        )\n",
    "\n",
    "        ######################################################\n",
    "        # Invoke\n",
    "        ######################################################\n",
    "        print(\"Invoking endpoints...\")\n",
    "        samples = load_dataset(\"sentence-transformers/stsb\", streaming=True, split=\"test\").take(500)\n",
    "\n",
    "        results = await asyncio.gather(\n",
    "            *[asyncio.to_thread(invoke_many, endpoint, samples) for _, endpoint in deployments]\n",
    "        )\n",
    "\n",
    "        ######################################################\n",
    "        # Analyze\n",
    "        ######################################################\n",
    "        print(\"Analyzing invocations...\")\n",
    "        for invocations in results:\n",
    "            latencies = np.array([invocation[\"latency\"] for invocation in invocations])\n",
    "            pprint.pp(\n",
    "                {\n",
    "                    \"shape\": np.shape(latencies),\n",
    "                    \"tm99\": np.mean(\n",
    "                        np.sort(latencies)[: (len(latencies) - int(len(latencies) * 0.01))]\n",
    "                    ),\n",
    "                    \"p90\": np.percentile(latencies, 90),\n",
    "                    \"avg\": np.mean(latencies),\n",
    "                    \"max\": np.max(latencies),\n",
    "                    \"min\": np.min(latencies),\n",
    "                    \"endpoint\": invocations[0][\"endpoint\"],\n",
    "                },\n",
    "                indent=4,\n",
    "            )\n",
    "    finally:\n",
    "        ######################################################\n",
    "        # Clean\n",
    "        ######################################################\n",
    "        print(\"Cleaning resources...\")\n",
    "        errors = await asyncio.gather(\n",
    "            *[asyncio.to_thread(clean, model, endpoint) for model, endpoint in deployments],\n",
    "            return_exceptions=True\n",
    "        )\n",
    "        for error in errors:\n",
    "            if error:\n",
    "                print(error)\n",
    "\n",
    "    print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Demo\n",
    "\n",
    "Note: To see live output from CloudWatch, please ensure your role has `logs:FilterLogEvents` permissions for the created endpoints\n",
    "\n",
    "Note: The logging output from the endpoints will be in <span style=\"background-color:#ffdddd\"> **red** because it is logged to stderr </span> by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying endpoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: INFO:     Skipping auto detection as the image uri is provided 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-cpu-py310-ubuntu22.04\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: DEBUG:     Uploading the model resources to bucket=sagemaker-us-west-2-987461069402, key_prefix=huggingface-pytorch-inference-2024-11-02-19-03-52-724.\n",
      "Uploading model artifacts:   0%|                                        | 0/7730 [00:00<?, ?bytes/s]ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "Uploading model artifacts: 100%|██████████████████████████| 7730/7730 [00:00<00:00, 53197.34bytes/s]\n",
      "ModelBuilder: DEBUG:     Model resources uploaded to: s3://sagemaker-us-west-2-987461069402/huggingface-pytorch-inference-2024-11-02-19-03-52-724/serve.tar.gz\n",
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "INFO:sagemaker:Creating model with name: transformers-1730574231\n",
      "INFO:sagemaker:Creating endpoint-config with name transformers-1730574231-2024-11-02-19-03-54-797\n",
      "ModelBuilder: INFO:     Unable to determine single GPU size for instance ml.c5.xlarge\n",
      "ModelBuilder: WARNING:     CUDA is not enabled on your device. Command '['nvidia-smi', '--query-gpu=name,memory.free', '--format=csv']' returned non-zero exit status 9.. Please run ModelBuilder on CUDA enabled hardware to deploy locally.\n",
      "ModelBuilder: WARNING:     60.21517667695886 percent of disk space used. Please consider freeing up disk space or increasing the EBS volume if you are on a SageMaker Notebook.\n",
      "ModelBuilder: WARNING:     60.21517667695886 percent of docker disk space at /var/lib/docker is used. Please consider freeing up disk space or increasing the EBS volume if you are on a SageMaker Notebook.\n",
      "ModelBuilder: INFO:     {'TS_DISABLE_TOKEN_AUTHORIZATION': 'true', 'HF_MODEL_ID': 'jinaai/jina-embeddings-v2-small-en'}\n",
      "INFO:sagemaker:Creating endpoint with name transformers-1730574231-2024-11-02-19-03-54-797\n",
      "ModelBuilder: INFO:     Detected 246618743249.dkr.ecr.us-west-2.amazonaws.com/tei-cpu:2.0.1-tei1.4.0-cpu-py310-ubuntu22.04. Proceeding with the the deployment.\n",
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "INFO:sagemaker:Creating model with name: tei-1730574231\n",
      "WARNING:sagemaker:Failed to enable live logging: An error occurred (AccessDeniedException) when calling the FilterLogEvents operation: User: arn:aws:sts::987461069402:assumed-role/AmazonSageMaker-ExecutionRole-20240417T141916/SageMaker is not authorized to perform: logs:FilterLogEvents on resource: arn:aws:logs:us-west-2:987461069402:log-group:/aws/sagemaker/Endpoints/transformers-1730574231-2024-11-02-19-03-54-797:log-stream: because no identity-based policy allows the logs:FilterLogEvents action. Fallback to default logging...\n",
      "INFO:sagemaker:Creating endpoint-config with name tei-1730574231-2024-11-02-19-03-56-898\n",
      "INFO:sagemaker:Creating endpoint with name tei-1730574231-2024-11-02-19-03-56-898\n",
      "WARNING:sagemaker:Failed to enable live logging: An error occurred (AccessDeniedException) when calling the FilterLogEvents operation: User: arn:aws:sts::987461069402:assumed-role/AmazonSageMaker-ExecutionRole-20240417T141916/SageMaker is not authorized to perform: logs:FilterLogEvents on resource: arn:aws:logs:us-west-2:987461069402:log-group:/aws/sagemaker/Endpoints/tei-1730574231-2024-11-02-19-03-56-898:log-stream: because no identity-based policy allows the logs:FilterLogEvents action. Fallback to default logging...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking endpoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: transformers-1730574231-2024-11-02-19-03-54-797\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: tei-1730574231-2024-11-02-19-03-56-898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing invocations...\n",
      "{   'shape': (500,),\n",
      "    'tm99': 0.021322881935375702,\n",
      "    'p90': 0.027897494498756715,\n",
      "    'avg': 0.021767640456022492,\n",
      "    'max': 0.17949875600061205,\n",
      "    'min': 0.013735082000494003,\n",
      "    'endpoint': 'tei-1730574231-2024-11-02-19-03-56-898'}\n",
      "{   'shape': (500,),\n",
      "    'tm99': 0.03614241738792367,\n",
      "    'p90': 0.04087031010003557,\n",
      "    'avg': 0.03672696926404751,\n",
      "    'max': 0.26481046400112973,\n",
      "    'min': 0.030846894998830976,\n",
      "    'endpoint': 'transformers-1730574231-2024-11-02-19-03-54-797'}\n",
      "Cleaning resources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: transformers-1730574231-2024-11-02-19-03-54-797\n",
      "INFO:sagemaker:Deleting endpoint with name: tei-1730574231-2024-11-02-19-03-56-898\n",
      "INFO:sagemaker:Deleting model with name: transformers-1730574231\n",
      "INFO:sagemaker:Deleting model with name: tei-1730574231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "await basic_performance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "This demo demonstrates:\n",
    "1. How to achieve custom pre/post-processing using the Transformers image alternative and customizing InferenceSpec\n",
    "2. Reduced, yet comparable performance versus the SageMaker TEI image while using the Transformers image. If pre/post-processing is needed for the TEI endpoint, then this is may be a viable alternative for production endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/generative_ai|sm-text_embedding_custom_processing.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/generative_ai|sm-text_embedding_custom_processing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
